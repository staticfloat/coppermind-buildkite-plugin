#!/bin/bash

COPPERMIND_REPO="$( cd "$( dirname "$( dirname "${BASH_SOURCE[0]}" )" )" &> /dev/null && pwd )"
source "${COPPERMIND_REPO}/lib/common.sh"

# Start hashing up our input glob patterns
INPUT_TREEHASHES=()
for PATTERN in "${INPUT_PATTERNS[@]}"; do
    INPUT_TREEHASHES+=( "$(collect_glob_pattern "${PATTERN}" | calc_treehash)" )
done

# Hash all treehashes together to get full input hash
export BUILDKITE_PLUGIN_COPPERMIND_INPUT_HASH=$(printf "%s" "${INPUT_TREEHASHES[@]}" | calc_shasum)

# Get listing of all files that exist with the given prefix


S3_ROOT="s3://${S3_BUCKET}/${S3_PREFIX}/${BUILDKITE_PLUGIN_COPPERMIND_INPUT_HASH}"
S3_LISTING=( $((AWS_ACCESS_KEY_ID=${BUILDKITE_S3_ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY=${BUILDKITE_S3_SECRET_ACCESS_KEY} aws s3 ls --recursive "${S3_ROOTDIR}" || true) | awk '{ $1=$2=$3=""; print $0 }' | sed 's/^[ \t]*//') )

if [[ ${#S3_LISTING[@]} -gt 0 ]]; then
    echo "Found ${#S3_LISTING[@]} previously uploaded artifacts"
    export BUILDKITE_PLUGIN_COPPERMIND_SKIP_COMMAND="yes"

    OUTPUT_GLOB="$(join_by "|" "${OUTPUT_PATTERNS[@]}")"
    for URL in "${S3_LISTING[@]}"; do
        # If this URL matches any part of our output glob, download it
        FILE_PATH="${URL#s3://${S3_BUCKET}/${S3_PREFOX}/}"
        if [[ "${FILE_PATH}" == @($OUTPUT_GLOB) ]]; then
            AWS_ACCESS_KEY_ID=${BUILDKITE_S3_ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY=${BUILDKITE_S3_SECRET_ACCESS_KEY} aws s3 cp "s3://${S3_BUCKET}/${URL}" .
        fi
    done
fi
